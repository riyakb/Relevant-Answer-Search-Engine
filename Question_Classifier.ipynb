{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question Classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riyakb/Relevant-Answer-Search-Engine/blob/master/Question_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "xK5M_eQ4VDnY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#mounting drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#changing path\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Relevant Answer Search Engine\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "47IOYOp701VY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install sner\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re, nltk\n",
        "import gensim\n",
        "import codecs\n",
        "from sner import Ner\n",
        "import spacy\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, average_precision_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk.internals import find_jars_within_path\n",
        "from nltk.tag import StanfordPOSTagger\n",
        "from nltk.tag import StanfordNERTagger\n",
        "import spacy\n",
        "from sklearn import linear_model\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import fbeta_score, accuracy_score\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ME7X-EIb01Vh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f_train = open('Question-Classification-master/traininig_dataset (1) (1).txt', 'r+')\n",
        "f_test = open('Question-Classification-master/validation_dataset (1) (1).txt', 'r+')\n",
        "\n",
        "train = pd.DataFrame(f_train.readlines(), columns = ['Question'])\n",
        "test = pd.DataFrame(f_test.readlines(), columns = ['Question'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3GkI8MR701Vl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train['QType'] = train.Question.apply(lambda x: x.split(' ', 1)[0])\n",
        "train['Question'] = train.Question.apply(lambda x: x.split(' ', 1)[1])\n",
        "train['QType-Coarse'] = train.QType.apply(lambda x: x.split(':')[0])\n",
        "train['QType-Fine'] = train.QType.apply(lambda x: x.split(':')[1])\n",
        "test['QType'] = test.Question.apply(lambda x: x.split(' ', 1)[0])\n",
        "test['Question'] = test.Question.apply(lambda x: x.split(' ', 1)[1])\n",
        "test['QType-Coarse'] = test.QType.apply(lambda x: x.split(':')[0])\n",
        "test['QType-Fine'] = test.QType.apply(lambda x: x.split(':')[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iHDUC9pg01V1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As can be observed, the train set consists of some duplicate question (81 to be exact). <br>\n",
        "The number of unique Coarse:Fine classes is 50 whereas entries corresponding to 42 are present in the test set. <br>\n",
        "The number of fine classes overall is 47 whereas entries corresponding to 39 are present in test."
      ]
    },
    {
      "metadata": {
        "id": "6dtY6lAc01V3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(pd.Series(train.QType.tolist() + test.QType.tolist()).values)\n",
        "train['QType'] = le.transform(train.QType.values)\n",
        "test['QType'] = le.transform(test.QType.values)\n",
        "le2 = LabelEncoder()\n",
        "le2.fit(pd.Series(train['QType-Coarse'].tolist() + test['QType-Coarse'].tolist()).values)\n",
        "train['QType-Coarse'] = le2.transform(train['QType-Coarse'].values)\n",
        "test['QType-Coarse'] = le2.transform(test['QType-Coarse'].values)\n",
        "le3 = LabelEncoder()\n",
        "le3.fit(pd.Series(train['QType-Fine'].tolist() + test['QType-Fine'].tolist()).values)\n",
        "train['QType-Fine'] = le3.transform(train['QType-Fine'].values)\n",
        "test['QType-Fine'] = le3.transform(test['QType-Fine'].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G77cGQEx01V8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_corpus = pd.Series(train.Question.tolist() + test.Question.tolist()).astype(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "68HUsvAJ01V_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Obtaining Dotwords.<br>\n",
        "Also, performing text cleaning and pre-processing in the next two blocks"
      ]
    },
    {
      "metadata": {
        "id": "JsUMdkMG01WA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer \n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "# dot_words = []\n",
        "# for row in all_corpus:\n",
        "#     for word in row.split():\n",
        "#         if '.' in word and len(word)>2:\n",
        "#             dot_words.append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-zg_B14G01WD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def text_clean(corpus, keep_list):\n",
        "    '''\n",
        "    Purpose : Function to keep only alphabets, digits and certain words (punctuations, qmarks, tabs etc. removed)\n",
        "    \n",
        "    Input : Takes a text corpus, 'corpus' to be cleaned along with a list of words, 'keep_list', which have to be retained\n",
        "            even after the cleaning process\n",
        "    \n",
        "    Output : Returns the cleaned text corpus\n",
        "    \n",
        "    '''\n",
        "    cleaned_corpus = pd.Series()\n",
        "    for row in corpus:\n",
        "        qs = []\n",
        "        for word in row.split():\n",
        "            if word not in keep_list:\n",
        "                p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n",
        "                p1 = p1.lower()\n",
        "                qs.append(p1)\n",
        "            else : qs.append(word)\n",
        "        cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
        "    return cleaned_corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XzUMGHbq01WG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess(corpus, keep_list, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n",
        "    \n",
        "    '''\n",
        "    Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n",
        "    \n",
        "    Input : \n",
        "    'corpus' - Text corpus on which pre-processing tasks will be performed\n",
        "    'keep_list' - List of words to be retained during cleaning process\n",
        "    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should \n",
        "                                                                  be performed or not\n",
        "    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n",
        "                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n",
        "    \n",
        "    Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n",
        "    \n",
        "    Output : Returns the processed text corpus\n",
        "    \n",
        "    '''\n",
        "    if cleaning == True:\n",
        "        corpus = text_clean(corpus, keep_list)\n",
        "    \n",
        "    if remove_stopwords == True:\n",
        "        wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "        stop = set(stopwords.words('english'))\n",
        "        for word in wh_words:\n",
        "            stop.remove(word)\n",
        "        corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
        "    else :\n",
        "        corpus = [[x for x in x.split()] for x in corpus]\n",
        "    \n",
        "    if lemmatization == True:\n",
        "        lem = WordNetLemmatizer()\n",
        "        corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
        "    \n",
        "    if stemming == True:\n",
        "        if stem_type == 'snowball':\n",
        "            stemmer = SnowballStemmer(language = 'english')\n",
        "            corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "        else :\n",
        "            stemmer = PorterStemmer()\n",
        "            corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    \n",
        "    corpus = [' '.join(x) for x in corpus]\n",
        "        \n",
        "\n",
        "    return corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4IWYzyG001WJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "common_dot_words = ['U.S.', 'St.', 'Mr.', 'Mrs.', 'D.C.']\n",
        "all_corpus = preprocess(all_corpus, keep_list = common_dot_words, remove_stopwords = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v5ExRr1U01WN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Splitting the preprocessed combined corpus again into train and test set"
      ]
    },
    {
      "metadata": {
        "id": "_mukv1lm6pLu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data=pd.read_csv('data.tsv',header=None,sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kR5rxOlpGfkn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data=train_data.loc[train_data[3] == 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YkcjPDqc01WO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_corpus = all_corpus[0:train.shape[0]]\n",
        "test_corpus = train_data[1].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kgr-mNAg01WR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Loading the English model for Spacy.<br>\n",
        "NLTK version for the same performs too slowly, hence opting for Spacy."
      ]
    },
    {
      "metadata": {
        "id": "uFLLujHH01WS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy download en\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TzYHGPal01WW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Obtaining Features from Train Data, which would be fed to CountVectorizer\n",
        "\n",
        "Creating list of Named Entitites, Lemmas, POS Tags, Syntactic Dependency Relation and Orthographic Features using shape.<br>\n",
        "Later, these would be used as features for our model."
      ]
    },
    {
      "metadata": {
        "id": "E-ohRPdU01WX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_ner = []\n",
        "all_lemma = []\n",
        "all_tag = []\n",
        "all_dep = []\n",
        "all_shape = []\n",
        "for row in train_corpus:\n",
        "    doc = nlp(row)\n",
        "    present_lemma = []\n",
        "    present_tag = []\n",
        "    present_dep = []\n",
        "    present_shape = []\n",
        "    present_ner = []\n",
        "    #print(row)\n",
        "    for token in doc:\n",
        "        present_lemma.append(token.lemma_)\n",
        "        present_tag.append(token.tag_)\n",
        "        #print(present_tag)\n",
        "        present_dep.append(token.dep_)\n",
        "        present_shape.append(token.shape_)\n",
        "    all_lemma.append(\" \".join(present_lemma))\n",
        "    all_tag.append(\" \".join(present_tag))\n",
        "    all_dep.append(\" \".join(present_dep))\n",
        "    all_shape.append(\" \".join(present_shape))\n",
        "    for ent in doc.ents:\n",
        "        present_ner.append(ent.label_)\n",
        "    all_ner.append(\" \".join(present_ner))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RdMwDIQI01Wb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Converting the attributes obtained above into vectors using CountVectorizer."
      ]
    },
    {
      "metadata": {
        "id": "yd3bMUV101Wc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "count_vec_ner = CountVectorizer(ngram_range=(1, 2)).fit(all_ner)\n",
        "ner_ft = count_vec_ner.transform(all_ner)\n",
        "count_vec_lemma = CountVectorizer(ngram_range=(1, 2)).fit(all_lemma)\n",
        "lemma_ft = count_vec_lemma.transform(all_lemma)\n",
        "count_vec_tag = CountVectorizer(ngram_range=(1, 2)).fit(all_tag)\n",
        "tag_ft = count_vec_tag.transform(all_tag)\n",
        "count_vec_dep = CountVectorizer(ngram_range=(1, 2)).fit(all_dep)\n",
        "dep_ft = count_vec_dep.transform(all_dep)\n",
        "count_vec_shape = CountVectorizer(ngram_range=(1, 2)).fit(all_shape)\n",
        "shape_ft = count_vec_shape.transform(all_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-9QMUlyA01We",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Combining the features obtained into 1 matrix"
      ]
    },
    {
      "metadata": {
        "id": "_ODQ7Gp401Wf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#x_all_ft_train = hstack([ner_ft, lemma_ft, tag_ft, dep_ft, shape_ft])\n",
        "x_all_ft_train = hstack([ner_ft, lemma_ft, tag_ft])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gqwsMBPD01Wj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Converting from COOrdinate format to Compressed Sparse Row format for easier mathematical computations."
      ]
    },
    {
      "metadata": {
        "id": "NMTUm9DQ01Wk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_all_ft_train = x_all_ft_train.tocsr()\n",
        "x_all_ft_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XzjGP_M201Wn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Now we will obtain the Feature vectors for the test set using the CountVectorizers Obtained from the Training Corpus"
      ]
    },
    {
      "metadata": {
        "id": "o-Rop0kn01Wo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_test_ner = []\n",
        "all_test_lemma = []\n",
        "all_test_tag = []\n",
        "all_test_dep = []\n",
        "all_test_shape = []\n",
        "i=0\n",
        "for row in test_corpus:\n",
        "    doc = nlp(row)\n",
        "    present_lemma = []\n",
        "    present_tag = []\n",
        "    present_dep = []\n",
        "    present_shape = []\n",
        "    present_ner = []\n",
        "    #print(row)\n",
        "    for token in doc:\n",
        "        present_lemma.append(token.lemma_)\n",
        "        present_tag.append(token.tag_)\n",
        "        #print(present_tag)\n",
        "        present_dep.append(token.dep_)\n",
        "        present_shape.append(token.shape_)\n",
        "    all_test_lemma.append(\" \".join(present_lemma))\n",
        "    all_test_tag.append(\" \".join(present_tag))\n",
        "    all_test_dep.append(\" \".join(present_dep))\n",
        "    all_test_shape.append(\" \".join(present_shape))\n",
        "    for ent in doc.ents:\n",
        "        present_ner.append(ent.label_)\n",
        "    all_test_ner.append(\" \".join(present_ner))\n",
        "    if i%500==0:\n",
        "      print(i)\n",
        "    i=i+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jL-AkuWd01Wq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ner_test_ft = count_vec_ner.transform(all_test_ner)\n",
        "lemma_test_ft = count_vec_lemma.transform(all_test_lemma)\n",
        "tag_test_ft = count_vec_tag.transform(all_test_tag)\n",
        "dep_test_ft = count_vec_dep.transform(all_test_dep)\n",
        "shape_test_ft = count_vec_shape.transform(all_test_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RPfolYvs01Ws",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#x_all_ft_test = hstack([ner_test_ft, lemma_test_ft, tag_test_ft, dep_test_ft, shape_test_ft])\n",
        "x_all_ft_test = hstack([ner_test_ft, lemma_test_ft, tag_test_ft])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6HwM4g6J01Wy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_all_ft_test = x_all_ft_test.tocsr()\n",
        "x_all_ft_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ctq4uTid01W1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Training\n",
        "Literature study over the years has shown Linear SVM performs best in this Use Case."
      ]
    },
    {
      "metadata": {
        "id": "R4rHiMTH01W2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = svm.LinearSVC()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Q9icYTY01W6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First Modelling for Coarse Classes"
      ]
    },
    {
      "metadata": {
        "id": "J0Ols25T01W7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_all_ft_train, train['QType-Coarse'].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "scxyXFgu01W9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "0cAsI53j01W-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preds = model.predict(x_all_ft_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W_BzL0dYP1m9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YE6e_nGbHgJi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dic=dict(zip(test_corpus, list(map(int, preds))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sdIBIUerHgCo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('qc.json', 'w') as fp:\n",
        "    json.dump(dic, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RO_KmlWm01Xp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "We achieved great accuracies using Feature Engineering as compared to accuracies obtained without feature engineering.\n",
        "(The notebook for models obtained without feature engineering is not being shared and one can try implementing it easily).\n",
        "\n",
        "Experimenting with informer hypernyms can further help in accuracy improvement as suggested in https://nlp.stanford.edu/courses/cs224n/2010/reports/olalerew.pdf"
      ]
    }
  ]
}